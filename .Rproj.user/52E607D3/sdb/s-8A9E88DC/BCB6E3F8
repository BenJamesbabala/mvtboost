{
    "contents" : "## Author: Patrick Miller\n## Purpose: Multiple response (or multivariate) boosting with decision trees (stumps). Also multi-task, or multi-objective. Only for continuous Y.\n## RRV: 5/19/2015\n  \n\n#' Fitting a Multivariate Tree Boosting Model\n#'\n#' Builds on gbm (Ridgeway 2013; Friedman, 2001) to fit a univariate tree model for each outcome, selecting predictors at each iteration that explain covariance between the outcomes. The number of trees included in the model can be chosen by minimizing the multivariate mean squared error using cross validation or a test set.\n#'\n#' @param X data frame of predictors\n#' @param Y data frame for outcome variables. For best performance, outcome variables should be scaled.\n#' @param n.trees maximum number of iterations (trees) to be included in the model\n#' @param shrinkage amount of shrinkage to be applied in each univariate model. This is the gbm shrinkage. Default is .01. Smaller shrinkage values require more iterations to ensure good fit.\n#' @param interaction.depth fixed depth of trees to be included in the model. A tree depth of 1 are stumps (main effects only), higher tree depths capture higher order interactions.\n#'\n#' @param bag.frac   proportion of the training sample used to fit univariate trees for each response at each iteration. Default: .5\n#' @param cv.folds   number of cross validation folds. Default: 1. For larger values, runs k + 1 models, where the k models are run in parallel.\n#' @param trainfrac  proportion of the sample used for training the multivariate additive model.\n#' @param samp.iter  T/F. If true, draws a new training sample at each iteration of the multivariate model. Default: FALSE.\n#'\n#' @param s vector of indices denoting observations to be used for the training sample\n#' @param seednum integer < 1000 to ensure that results are reproducible\n#' @param compress T/F. Compress output results list using bzip2 (approx 10\\% of original size). Default FALSE.\n#' @param save.cv  T/F. Save all k cv models.\n#' @param alpha optional argument to select predictors that explain more variance or covariance in outcomes. Variance reductions are weighted by alpha, and covariance reductions are weighted by 1-alpha.\n#' @param weight.type see Details.\n#' @param cov.discrep see Details.\n#' @return Fitted model. This is a list containing the following elements:\n#' \n#' \\itemize{\n#'   \\item covex - covariance explained in each pair of outcomes by each predictor, in original order.\n#'   \\item weights - relative weights at each iteration\n#'   \\item wm - raw values of the loss function at each iteration (optionally, truncated at zero)\n#'   \\item maxiter - maximum number of iterations run\n#'   \\item best.iters - list of the best best iteratino (min MSE test, min MSE cv, last)\n#'   \\item finaltree - list of trees in the final model\n#'   \\item trainerr - vector of the MSE in the training set at each iteration\n#'   \\item testerr - vector of MSE in test set at each iteration\n#'   \\item params - arguments\n#'   \\item iter.models - list of q gbm models from the first 5 iterations and last.\n#'   \\item bestxs - vector of predictors selected at each iteration\n#'   \\item bestys - vector of dependent variables selected at each iteration\n#'   \\item resid - n x q matrix of residuals after all iterations\n#'   \\item init - colMeans(Y)\n#'   \\item s - indices of training sample\n#'   \\item n - number of observations\n#'   \\item ocv - if save.cv=TRUE, returns the CV models.\n#'   \\item uncomp - function to uncompress the results if necessary.\n#' }\n#' \n#' @details Note that this is a beta version with details subject to change. Any contributions are welcome.\n#' \n#' @references Miller P.J., Lubke G.H, McArtor D.B., Bergeman C.S. (2015) Finding structure in data: A data mining alternative to multivariate multiple regression. Psychological Methods.\n#' @export\nmvtb <- function(X=X,Y=Y,n.trees=100,shrinkage=.01,interaction.depth=1,\n                 trainfrac=1,samp.iter=FALSE,bag.frac=1,cv.folds=1,\n                 s=NULL,seednum=NULL,compress=FALSE,save.cv=FALSE,alpha=.5,cov.discrep=1,weight.type=1,...) {\n  \n  params <- c(as.list(environment()))\n  \n  ## seeds\n  if(!is.null(seednum)){\n    #print(c(\"mvtboost: seednum \",seednum));\n    set.seed(seednum)\n  }\n  \n  ## Data\n  n <- nrow(X); k <- ncol(Y); p <- ncol(X);\n  if(is.null(colnames(X))) { colnames(X) <- paste(\"X\",1:p,sep=\"\") }\n  if(is.null(colnames(Y))) { colnames(Y) <- paste(\"Y\",1:k,sep=\"\") } \n  xnames <- colnames(X)\n  ynames <- colnames(Y)\n  D <- Rm <- matrix(0,n,k)            \n  \n  ## sampling\n  if(is.null(s)){\n    ## training sample\n    params$s <- s <- sample(1:n,floor(n*trainfrac),replace=F) #force round down if odd\n  } \n  \n  ## parameters\n  plist <- params\n  plist$cov.discrep <- NULL\n  plist$alpha       <- NULL\n  plist$weight.type <- NULL\n  \n  ## Checks\n  if(any(is.na(Y))){ stop(\"NAs not allowed in response variables.\")}\n    \n  ## Influence\n  wm.raw <- wm.rel <- matrix(0,nrow=n.trees,ncol=k)    #raw, relative\n  rel.infl <- w.rel.infl <- array(0,dim=c(p,k,n.trees)) # influences at every iteration\n  \n  ## Iterations\n  trainerr <- testerr <- bestys <- bestxs <- vector(length=n.trees)\n  #bestxs <- vector(mode=\"list\",length=n.trees)\n  final.iter <- FALSE\n  \n  ## Covex\n  Res.cov <- array(0,dim=c(k,k,k,n.trees))\n  covex <- array(0,dim=c(p,k*(k+1)/2))\n  rownames(covex) <- colnames(X)\n  names <- outer(1:k,1:k,function(x,y){paste0(colnames(Y)[x],\"-\",colnames(Y)[y])})\n  colnames(covex) <- names[lower.tri(names,diag=TRUE)]\n  ## Note that this is unknown here, for GLS.\n  #if(loss.function==4) { Sinv <- solve(cov(as.matrix(Y[s,]))) }\n\n  \n  ## 0. CV?\n  if(cv.folds > 1) {\n    ocv <- mvtbCV(params=plist)\n    best.iters.cv <- ocv$best.iters.cv\n    cv.err <- ocv$cv.err\n    last <- ocv$models.k[[cv.folds+1]]\n    last$ocv <- NULL\n    last$X <- X\n    last$Y <- Y\n    list2env(last,envir=environment())\n    #best.iters <- as.list(data.frame(best.testerr=which.min(testerr),best.iters.cv[1],last=i))\n  } else {\n    out.mvtb <- do.call(\"mvtb.fit\",args=c(plist))\n    list2env(out.mvtb,envir=environment()) # adds models, trainerr, testerr, s, ss, and yhat to the current environment.\n    best.iters.cv <- NULL\n    cv.err <- NULL\n    ocv <- NULL\n  }\n  init <- unlist(lapply(models,function(m){m$initF}))\n  for(m in 1:k) { D[,m] <- Y[,m]-init[m] } # current residuals at iteration 1\n  yhat <- array(c(rep(init,each=n),yhat),dim=c(n,k,n.trees+1))\n  Dpred <- yhat[,,2:(n.trees+1),drop=F]-yhat[,,1:(n.trees),drop=F] # Dpred is the unique contribution of each tree\n  \n  # 1. Get trees from each model\n  finaltree <- list()\n  for(m in 1:k) { \n    finaltree[[m]] <- models[[m]]$trees\n  }\n  # 2. Compute covariance discrepancy\n  for(i in 1:(n.trees)) {        \n    s <- ss[,i]\n    \n    ## 2.1 From each model get the stuff we need at the current iteration\n    for(m in 1:k) {            \n      ## 1.2 For each model compute predicted values and influence\n      tree.i <- finaltree[[m]][i]\n      rel.infl[,m,i] <- ri.one(tree.i,n.trees=1,xnames)\n      ## 1.3 Replace mth outcome with its residual, compute covariance           \n      Rm <- D \n      Rm[,m] <- D[,m]-Dpred[,m,i]\n      Res.cov[,,m,i] <- cov(as.matrix(Rm),use=\"pairwise.complete\")            \n      ## 2. Evaluate loss criteria on training sample. Covariance reduced, correlation reduced, uls, or gls\n      wm.raw[i,m] <- eval.loss(Rm=as.matrix(Rm[s,]),D=as.matrix(D[s,]),alpha=alpha,type=cov.discrep)\n    }              \n    \n    wm.raw[i,wm.raw[i,] < 0] <- 0 # truncate negative weights to 0\n    \n    ## 3. Check early stopping criteria.\n    if(all(wm.raw[i,]==0)) {\n      wm.rel[i,] <- 0\n      final.iter <- TRUE; # break at the end of the loop, so test and train error can still be computed.\n    }                     \n    \n    ## 4. Compute weight types (relative, 0-1, and none)\n    if(!final.iter) {\n      if(weight.type==1) {\n        ##print(\"relative weights\")\n        wm.rel[i,] <- wm.raw[i,]/sum(wm.raw[i,]) # relative weights.\n      } else if(weight.type==2) {\n        wm.rel[i,which.max(wm.raw[i,])] <- 1 # 0-1 weights for covariance explained loss functions (want to maximize)       \n      } else {\n        ##print(\"equal weight\")\n        wm.rel[i,] <- rep(1,k) # equal weight\n      }\n    }\n    # compute the best mod, and which xs were selected\n    besty <- bestys[i] <- which.max(wm.raw[i,])           \n    bestxs[i] <- bestx <- which.max(rel.infl[,besty,i])    \n    \n    # compute the covariance reduced by the best predictor\n    Sd <- cov(D)-Res.cov[,,besty,i]\n    covex[bestx,] <- covex[bestx,] + Sd[lower.tri(Sd,diag=TRUE)]         \n    \n    #Rm <- D - Dpred[,,i]\n    D <- Rm\n    \n    ## 7. Compute best iteration criteria: training and test error, sum of residual covariace matrix, weighted sum of residual covariance matrix\n    #trainerr[i] <- mean((as.matrix(Rm[s,]))^2,na.rm=TRUE)\n    #if(length(unique(s)) == nrow(P)) { # use training error instead of testing\n    #  testerr[i] <- trainerr[i]            \n    #} else {\n    #  testerr[i] <- mean((as.matrix(Rm[-s,]))^2,na.rm=TRUE)\n    #}\n    \n    if(final.iter) {\n      i <- i-1 # don't count the last iteration if all weights are  0\n      break;\n    }\n  }\n  ## 8. Compute weighted influences\n  for(m in 1:k) {\n    #rel.infl[,m] <- relative.influence(models[[m]],n.trees=i,scale=FALSE)\n    w.rel.infl[,m,i] <- rel.infl[,m,i]*wm.rel[i,m]\n  }\n  \n  best.trees <- list(best.testerr=which.min(testerr),best.cv=best.iters.cv,last=i)\n\n  covex <- t(covex)\n  if(!save.cv){ocv <- NULL}\n\n  fl <- list(models=models, covex=covex,maxiter=i,best.trees=best.trees,\n             rel.infl=rel.infl, w.rel.infl=w.rel.infl,params=params,\n             trainerr=trainerr[1:i],testerr=testerr[1:i],cv.err=cv.err[1:i],\n             bestxs=bestxs,bestys=bestys,\n             resid=Rm,ocv=ocv,\n             wm.raw=matrix(wm.raw[1:i,,drop=FALSE],nrow=i,ncol=k),wm.rel=wm.rel[1:i,,drop=FALSE],\n             s=ss,n=nrow(X),xnames=colnames(X),ynames=colnames(Y))\n  if(compress) {\n    # compress each element using bzip2\n    fl <- lapply(fl,comp)\n  }\n  return(fl)\n}\n\n\nmvtb.fit <- function(X,Y,n.trees=100,shrinkage=.01,interaction.depth=1,\n                           trainfrac=1,samp.iter=FALSE,bag.frac=1,cv.folds=1,\n                           s=NULL,seednum=NULL,compress=FALSE,save.cv=FALSE,...) {\n    if(!is.null(seednum)){\n      #print(c(\"mvtboost: seednum \",seednum));\n      set.seed(seednum)\n    }\n\n    ## 0. Generate random training samples. This is done before the fitting for reproducibility.\n    n <- nrow(X) \n    q <- ncol(Y)\n\n    ss <- matrix(0,nrow=length(s),ncol=n.trees)\n    \n    ## 1. Sampling with each iteration, if desired.\n    for(i in 1:n.trees) {\n        if(samp.iter) {\n            ss[,i] <-  sample(s,length(s),replace=TRUE) # if replace = FALSE, this just permutes the rows.\n        } else {\n            ss[,i] <- s\n        }\n    }  \n    \n    ## 2. Fit the models\n    models <- pred <- list()\n    for(m in 1:q) {\n      models[[m]] <- gbm.fit(x=as.data.frame(X[s,,drop=FALSE]),y=matrix(Y[s,m,drop=FALSE]),\n                              shrinkage=shrinkage,interaction.depth=interaction.depth,\n                              n.trees=n.trees,verbose=F,distribution=\"gaussian\",\n                              bag.fraction=bag.frac,keep.data=FALSE,...)\n    }\n    ## Multivariate mse. clumsy, I know.\n    yhat <- predict.mvtb(list(models=models),n.trees=1:n.trees,newdata=X)\n    testerr <- trainerr <- rep(0,length=n.trees)\n    for(i in 1:n.trees){\n      R <- Y-yhat[,,i]\n      trainerr[i] <- mean((as.matrix(R[s,]))^2,na.rm=TRUE)\n      testerr[i] <- mean((as.matrix(R[-s,]))^2,na.rm=TRUE)\n    }\n    \n    ## 3. Compute multivariate MSE\n    fl <- list(models=models,trainerr=trainerr,testerr=testerr,yhat=yhat,s=s,ss=ss)\n    return(fl)\n}\n\nri.one <- function(object,n.trees=1,var.names=xnames) {\n  get.rel.inf <- function(obj) {\n    lapply(split(obj[[6]], obj[[1]]), sum)\n  }\n  temp <- unlist(lapply(object[1:n.trees], get.rel.inf))\n  rel.inf.compact <- unlist(lapply(split(temp, names(temp)), \n                                   sum))\n  rel.inf.compact <- rel.inf.compact[names(rel.inf.compact) != \n                                       \"-1\"]\n  rel.inf <- rep(0, length(var.names))\n  i <- as.numeric(names(rel.inf.compact)) + 1\n  rel.inf[i] <- rel.inf.compact\n  return(rel.inf = rel.inf)\n}\n\neval.loss <- function(Rm,D,alpha,type) {\n    ## Rm = n x k matrix of residuals\n    ## D = n x k matrix of Ytilde at iteration i\n    ## S = k x k covariance matrix of Ytilde at iteration i\n    ## Res.cov = k x k covariance matrix of Rm (residuals)\n    ## Sd = S - Res.cov, k x k the discrepancy between S and Res.cov. The larger the discrepancy, the more covariance is explained.\n    Res.cov <- cov(as.matrix(Rm),use=\"pairwise.complete\")\n    S <- cov(as.matrix(D),use=\"pairwise.complete\")\n    Sd <- S-Res.cov\n    ## type is an integer 1:5, maps to loss function 1, 2, ... etc.\n    switch(type,\n           cov.red(S=S,Res.cov=Res.cov,alpha=alpha),\n           cor.red(Rm=Rm,D=D),\n           uls(Sd=Sd),\n           #gls(Sd=Sd,Sinv=Sinv),\n           detcov(S,Res.cov))\n}\n\ndetcov <- function(S,Res.cov) {\n    det(S)-det(Res.cov)\n}\n\n## Covariance explained, weighted by alpha. alpha*diagonal (1-alpha)*off-diagonal\ncov.red <- function(S,Res.cov,alpha) {\n    Sd <- abs(S-Res.cov)\n    alpha*sum(diag(Sd),na.rm=TRUE) + (1-alpha)*sum(Sd[lower.tri(Sd)],na.rm=TRUE)\n}\n\ncor.red <- function(Rm,D) {    \n    if(ncol(Rm) == 1) {\n        wm <- 1-cor(Rm,D,use=\"pairwise.complete\")\n    } else {\n        Sd <- abs(cor(D,use=\"pairwise.complete\") - cor(Rm,use=\"pairwise.complete\"))\n        wm <- sum(Sd[upper.tri(Sd)],na.rm=TRUE)\n    }\n    return(wm)\n}\n\n## Some covariance discrepancy functions, motivated from SEM.\n## 1/2 the ||R||_F, the frobenius norm of the  covariance discrepancy matrix\nuls <- function(Sd) { .5 * sum(diag(Sd %*% t(Sd))) }\n\n## generalized least squares. requires precomputed inverse of the sample covariance matrix of the responses.\n#gls <- function(Sd,Sinv) { .5 * sum(diag( (Sd %*% Sinv) %*% t(Sd %*% Sinv) ),na.rm=TRUE) }\n\ncomp <- function(obj) { memCompress(serialize(obj,NULL),\"bzip2\") }\nuncomp <-function(obj){ unserialize(memDecompress(obj,type=\"bzip2\"))}\n\n## the only tricky thing here is that some models can stop at earlier iterations than others.\n## The default error for each is NA, and the average error (rowMeans over k) is computed with na.rm=TRUE.\n## Thus the error estimate at later iterations may not be based on all folds.\nmvtbCV <- function(params) {\n    n <- nrow(params$X)\n    cv.folds <- params$cv.folds\n    save.cv <- params$save.cv\n    \n    #if(is.null(params$s)) {\n    #    s <- sample(1:n,floor(n*params$trainfrac))\n    #}\n    s <- params$s\n    cv.groups <- sample(rep(1:cv.folds,length=length(s)))\n\n    # construct the new call\n    params$trainfrac <- 1\n    params$cv.folds <- 1\n    params$save.cv <- save.cv\n    params$X <- params$X[s,, drop=FALSE]\n    params$Y <- params$Y[s,, drop=FALSE]\n\n    testerr.k <- matrix(NA,nrow=params$n.trees,ncol=cv.folds)\n    out.k <- list()\n\n    runone <- function(k,params,cv.groups){\n      if(any(k %in% cv.groups)) {\n        params$s <- which(cv.groups != k)\n      } else { \n        # since we already subsetted on s\n        params$s <- 1:nrow(params$X) \n        params$compress <- FALSE\n      }\n      out <- do.call(\"mvtb.fit\",params)  \n      return(out)\n    }\n    # Last fold contains the full sample\n    # out.k <- mclapply(1:(cv.folds+1),runone,params=params,cv.groups=cv.groups,mc.cores=cv.folds)\n     out.k <- lapply(1:(cv.folds+1),runone,params=params,cv.groups=cv.groups)\n        \n    for(k in 1:cv.folds) {\n      out <- out.k[[k]]\n      testerr.k[,k] <- out$testerr\n    }\n  \n    cv.err <- rowMeans(testerr.k,na.rm=TRUE)\n    best.iters.cv <- which.min(cv.err)\n    \n \n    if(params$save.cv) {\n        l <- list(models.k=out.k,best.iters.cv=best.iters.cv,cv.err=cv.err,cv.groups=cv.groups)\n    } else {\n        out.k[1:cv.folds] <- NULL\n        models.k <- c(lapply(1:cv.folds,list),out.k)\n        l <- list(models.k=models.k,best.iters.cv=best.iters.cv,cv.err=cv.err)\n    }\n    return(l)\n}\n\n### Examples ###\n\n#assign default values of arguments to workspace\n## list2env(formals(mvtb)[-c(1:2)],globalenv())\n#  params <- formals(mvtb)[-c(length(formals(mvtb)))]\n# \n# B <- matrix(c(.5,.5,0,0,.5,.5),ncol=3,byrow=TRUE)\n# B\n# varx <- 2\n# vare <- diag(3)\n# #diag(vare) <- 1:3\n# vare[lower.tri(vare)] <- vare[upper.tri(vare)] <- 0\n# \n# n <- 1000\n# X <- matrix(rnorm(n*2,1,2),ncol=2)\n# E <- mvrnorm(n,mu=c(0,0,0),Sigma=vare)\n# Y <- X %*% B + E\n# bhat <- solve(t(X) %*% X) %*% t(X) %*% Y\n# cov(Y)\n# cov(X %*% B)\n# colnames(X) <- paste0(\"X\",1:varx)\n# params$X <- X\n# params$Y <- Y\n\n#out <- mvtb(X=X,Y=Y,shrinkage=1,n.trees=200,weight.type=2,weighted.predictions=TRUE)\n#out$covex\n#out$rel.covex\n\n## I think it works!\n\n#' Predicted values\n#' @param out mvtb object\n#' @param n.trees number of trees. If a list, returns predictions in a \n#' @param newdata new matrix of predictors.\n#' @return Returns a matrix of predictions for each outcome. \n#' If n.trees is a vector, returns an array, where the third dimension corresponds to the \n#' predictions at a given number of trees.\n#' @export\npredict.mvtb <- function(out, n.trees, newdata) {\n  K <- length(out$models)\n  treedim <- ifelse(length(n.trees) > 1,max(n.trees),1)\n  Pred <- array(0,dim=c(nrow(newdata),K,treedim))  \n  for(k in 1:K) {                                     \n    p <- rep(0,nrow(newdata))        \n    p <- predict(out$models[[k]],n.trees=n.trees,newdata=newdata)    \n    Pred[,k,] <- p\n  }\n  #if(length(n.trees) == 1) {\n  #  Pred <- drop(Pred)\n  #}\n  return(Pred)\n}\n",
    "created" : 1432216757647.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2937695391",
    "id" : "BCB6E3F8",
    "lastKnownWriteTime" : 1432214911,
    "path" : "~/Software/mvtb2_package/mvtboost/R/mvtboost.R",
    "project_path" : "R/mvtboost.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}